---
title: "R Notebook"
author: Group YoutubeFinder
output: html_notebook
---
# Analytics of the Video Metadata datasets
**Obs**: All the analysis performed throughout this R markdown is better explained in the written document. There it is detailed the motivations and reasoning for each process.

```{r}
library("readtext")
library("jsonlite")
library("rjson")
library("parallel")
library("ggplot2")
library("reshape2")

# This is a temporary solution to read the files from local repository
# Later they should be read from HDFS
# File location: https://drive.google.com/file/d/1ERz6um0Wy2Qzjh76AlHZ70dt1Dc26JZJ/view?usp=sharing
# Download and save in local machine. Replace the root_path directory.
root_path <- "/Users/daniel/LocalFiles for TFM/"
setwd(root_path)

#Reading the files using parallel (takes on average 1h30min to finish)
cl <- makeCluster(detectCores() - 1)
#Parallel not functioning in RStudio https://github.com/rstudio/rstudio/issues/6692
json_files<-list.files(path =paste0(root_path,"Files/video-content/"),pattern="*.json",full.names = TRUE)
json_list<-parLapply(cl,json_files,function(x) rjson::fromJSON(file=x,method = "R"))
stopCluster(cl)

class(json_list)
# list
length(json_list)
# 72068
lengths(json_list)
# Each list with 64 elements
```

# Calculating **Missing Values** for the main elements in video metadata
```{r}
library("dplyr")
library("tidyr")

#Function works only for list type variables
function_is_null <- function(x, var = " "){
  len <- length(x)
  null = 0
  for(i in 1:len){
    if (is.null(x[[i]][[var]])){
      null <- null + 1
    }
  }
  cat(var,i)
  return(null)
}

null_dislike_count <- function_is_null(json_list,var = "dislike_count")
null_like_count <- function_is_null(json_list,var = "like_count")
null_id_count_count <- function_is_null(json_list,var = "id")
null_channel_id_count <- function_is_null(json_list,var = "channel_id")
null_categories_count <- function_is_null(json_list,var = "categories")
null_description_count <- function_is_null(json_list,var = "description")
null_webpage_url_count <- function_is_null(json_list,var = "webpage_url")
null_upload_date_count <- function_is_null(json_list,var = "upload_date")
null_average_rating_count <- function_is_null(json_list,var = "average_rating")
null_view_count <- function_is_null(json_list,var = "view_count")
null_thumbnail_count <- function_is_null(json_list,var = "thumbnail")
null_duration_count <- function_is_null(json_list,var = "duration")
null_fulltitle_count <- function_is_null(json_list,var = "fulltitle")

col_names <- c("Dislike Count","Like Count","Video ID", "Channel ID","Categories","Description", "Webpage URL",
               "Upload Date","Average Rating", "View Count","Thumbnail","Duration","Full Title")
df <- data.frame(null_dislike_count,null_like_count,null_id_count_count,null_channel_id_count,null_categories_count,null_description_count,null_webpage_url_count,
          null_upload_date_count,null_average_rating_count,null_view_count,null_thumbnail_count,null_duration_count,null_fulltitle_count)
colnames(df) <- col_names

# Plotting the results
library("ggplot2")
library("reshape2")

cd274 <- melt(df[, 1:ncol(df)])
ggplot(cd274, aes(x = variable, y = value)) + geom_bar(stat = "identity",position = "dodge") +
  labs(x='variable', y="number of missing values", title='Missing Values Analysis Main Variables') +
  coord_flip() + geom_text(aes(label=value))
```

# Language Detection Analysis

## Test CLD3 package
```{r}
library("cld3")

#Function works only for list type variables
fun_transf_df_language_detection_cld3 <- function(x,var1 = " ",var2=" "){
  len <- length(x)
  for(i in 1:len){
    text1<-x[[i]][[var1]]
    text2<-x[[i]][[var2]]
    combinedText<-paste(text1,text2)
    language <- cld3::detect_language(combinedText)
    if(i == 1){
      dataframe <- data.frame(combinedText,language)
      col <- c("Text","Language_Detected")
      colnames(dataframe) <- col
    } else{
      dataframe[nrow(dataframe)+1,] <- cbind(combinedText,language)
    }
  }
  return(dataframe)
}

df_language_fulltitle <- fun_transf_df_language_detection_cld3(json_list,"fulltitle","description")
summary_cld3 <- df_language_fulltitle %>%
  group_by(Language_Detected) %>%
  summarise(count=n()) %>% arrange(desc(count))
```

## Test Texcat package
```{r}
library("textcat")
fun_transf_df_language_detection_textcat <- function(x,var1 = " ",var2=" "){
  len <- length(x)
  for(i in 1:len){
    text1<-x[[i]][[var1]]
    text2<-x[[i]][[var2]]
    combinedText<-paste(text1,text2)
    language <- textcat::textcat(combinedText,p=textcat::TC_char_profiles)
    if(i == 1){
      dataframe <- data.frame(combinedText,language)
      col <- c("Text","Language_Detected")
      colnames(dataframe) <- col
    } else{
      dataframe[nrow(dataframe)+1,] <- cbind(combinedText,language)
    }
  }
  return(dataframe)
}

df_language_fulltitle_textcat <- fun_transf_df_language_detection_textcat(json_list,"fulltitle","description")
summary_textcat <- df_language_fulltitle_textcat %>%
  group_by(Language_Detected) %>%
  summarise(count=n()) %>% arrange(desc(count))
```

```{r}
# Testing the results
sample <- sample(1:length(json_list),round(0.0001*length(json_list)))
for(i in 1:length(sample)){
  if(i == 1){
      test_result <- data.frame(sample[i],json_list[[sample[i]]][["description"]])
      col <- c("RowId","Text")
      colnames(test_result) <- col
    } else{
      test_result[nrow(test_result)+1,] <- cbind(sample[i],json_list[[sample[i]]][["description"]])
    }
}
```

# Analysis to find out how the datasets are balanced
```{r}
definitive_ids <- read.delim(paste0(root_path,"Files/definitive_videoids_updated_20200901.txt"),header=FALSE,sep=" ",dec=" ")
beauty_fitness <- read.delim(paste0(root_path,"Files/Beauty_Fitness.txt"),header=FALSE,sep=" ",dec=" ")
books_literature <- read.delim(paste0(root_path,"Files/Books_Literature.txt"),header=FALSE,sep=" ",dec=" ")
business_industrial <- read.delim(paste0(root_path,"Files/Business_Industrial.txt"),header=FALSE,sep=" ",dec=" ")
computers_electronics <- read.delim(paste0(root_path,"Files/Computers_Electronics.txt"),header=FALSE,sep=" ",dec=" ")
food_drink <- read.delim(paste0(root_path,"Files/Food_Drink.txt"),header=FALSE,sep=" ",dec=" ")

beauty_fitness$check<- ifelse(beauty_fitness$V1 %in% definitive_ids$V1,"Yes", "No")
new_beauty_fitness <- beauty_fitness %>% filter(check == "Yes")

books_literature$check<- ifelse(books_literature$V1 %in% definitive_ids$V1,"Yes", "No")
new_books_literature <- books_literature %>% filter(check == "Yes")

business_industrial$check<- ifelse(business_industrial$V1 %in% definitive_ids$V1,"Yes", "No")
new_business_industrial <- business_industrial %>% filter(check == "Yes")

computers_electronics$check<- ifelse(computers_electronics$V1 %in% definitive_ids$V1,"Yes", "No")
new_computers_electronics <- computers_electronics %>% filter(check == "Yes")

food_drink$check<- ifelse(food_drink$V1 %in% definitive_ids$V1,"Yes", "No")
new_food_drink <- food_drink %>% filter(check == "Yes")

result_balance<-data.frame(nrow(new_beauty_fitness),nrow(new_books_literature),nrow(new_business_industrial),nrow(new_computers_electronics),nrow(new_food_drink))
colnames(result_balance) <- c("Beauty & Fitness","Books & Literature", "Business & Industrial","Computers & Electronics","Food & Drink")
View(result_balance)

# Check how many Ids are present in all categories and which are exclusive per category
'%notin%' <- Negate('%in%')
definitive_ids$check_all <- ifelse((definitive_ids$V1 %in% food_drink$V1 &
                                               definitive_ids$V1 %in% computers_electronics$V1 &
                                               definitive_ids$V1 %in% business_industrial$V1 &
                                               definitive_ids$V1 %in% books_literature$V1 &
                                               definitive_ids$V1 %in% beauty_fitness$V1),"Yes", "No")

definitive_ids$exclusive_fooddrink <- ifelse(definitive_ids$V1 %in% food_drink$V1 &
                                               definitive_ids$V1 %notin% computers_electronics$V1 &
                                               definitive_ids$V1 %notin% business_industrial$V1 &
                                               definitive_ids$V1 %notin% books_literature$V1 &
                                               definitive_ids$V1 %notin% beauty_fitness$V1,"Yes", "No")

definitive_ids$exclusive_compelect<- ifelse(definitive_ids$V1 %notin% food_drink$V1 &
                                               definitive_ids$V1 %in% computers_electronics$V1 &
                                               definitive_ids$V1 %notin% business_industrial$V1 &
                                               definitive_ids$V1 %notin% books_literature$V1 &
                                               definitive_ids$V1 %notin% beauty_fitness$V1,"Yes", "No")

definitive_ids$exclusive_busind<- ifelse(definitive_ids$V1 %notin% food_drink$V1 &
                                               definitive_ids$V1 %notin% computers_electronics$V1 &
                                               definitive_ids$V1 %in% business_industrial$V1 &
                                               definitive_ids$V1 %notin% books_literature$V1 &
                                               definitive_ids$V1 %notin% beauty_fitness$V1,"Yes", "No")

definitive_ids$exclusive_bookslit<- ifelse(definitive_ids$V1 %notin% food_drink$V1 &
                                               definitive_ids$V1 %notin% computers_electronics$V1 &
                                               definitive_ids$V1 %notin% business_industrial$V1 &
                                               definitive_ids$V1 %in% books_literature$V1 &
                                               definitive_ids$V1 %notin% beauty_fitness$V1,"Yes", "No")

definitive_ids$exclusive_beautyfit<- ifelse(definitive_ids$V1 %notin% food_drink$V1 &
                                               definitive_ids$V1 %notin% computers_electronics$V1 &
                                               definitive_ids$V1 %notin% business_industrial$V1 &
                                               definitive_ids$V1 %notin% books_literature$V1 &
                                               definitive_ids$V1 %in% beauty_fitness$V1,"Yes", "No")

exclusive_balance<-data.frame(nrow(definitive_ids %>% filter(check_all == "Yes")),
                              nrow(definitive_ids %>% filter(exclusive_fooddrink == "Yes")),
                              nrow(definitive_ids %>% filter(exclusive_compelect == "Yes")),
                              nrow(definitive_ids %>% filter(exclusive_busind == "Yes")),
                              nrow(definitive_ids %>% filter(exclusive_bookslit == "Yes")),
                              nrow(definitive_ids %>% filter(exclusive_beautyfit == "Yes")))
colnames(exclusive_balance) <- c("In All Cats","Food & Drink","Computers & Electronics","Business & Industrial","Books & Literature","Beauty & Fitness")

###
definitive_ids$inbeautyfitness<- ifelse(definitive_ids$V1 %in% beauty_fitness$V1,"Yes", "No")
new_beauty_fitness <- definitive_ids %>% filter(inbeautyfitness == "Yes")

definitive_ids$inbooksliterature<- ifelse(definitive_ids$V1 %in% books_literature$V1,"Yes", "No")
new_books_literature <- definitive_ids %>% filter(inbooksliterature == "Yes")

definitive_ids$inbusinessindustrial<- ifelse(definitive_ids$V1 %in% business_industrial$V1,"Yes", "No")
new_business_industrial <- definitive_ids %>% filter(inbusinessindustrial == "Yes")

definitive_ids$incomputer<- ifelse(definitive_ids$V1 %in% computers_electronics$V1,"Yes", "No")
new_computers_electronics <- definitive_ids %>% filter(incomputer == "Yes")

definitive_ids$infooddrink<- ifelse(definitive_ids$V1 %in% food_drink$V1,"Yes", "No")
new_food_drink <- definitive_ids %>% filter(infooddrink == "Yes")

third_balance_test<-data.frame(nrow(new_beauty_fitness),nrow(new_books_literature),nrow(new_business_industrial),nrow(new_computers_electronics),nrow(new_food_drink))
colnames(third_balance_test) <- c("Beauty & Fitness","Books & Literature", "Business & Industrial","Computers & Electronics","Food & Drink")
View(third_balance_test)

melted <- melt(third_balance_test[, 1:ncol(third_balance_test)])
ggplot(melted, aes(x = variable, y = value)) + geom_bar(stat = "identity",position = "dodge") +
  labs(x='YouTube-8M defined category name', y="number of video ids randomly selected", title='Number of IDs selected by Category in YouTube-8M') +
  coord_flip() + geom_text(aes(label=value))
```

# Checking the total number of video ids per category from the **Original files in Youtube-8M**
```{r}
original_datasets_balance<-data.frame(nrow(beauty_fitness),nrow(books_literature),nrow(business_industrial),nrow(computers_electronics),nrow(food_drink))
colnames(original_datasets_balance) <- c("Beauty & Fitness","Books & Literature", "Business & Industrial","Computers & Electronics","Food & Drink")
View(original_datasets_balance)

melted <- melt(original_datasets_balance[, 1:ncol(original_datasets_balance)])
ggplot(melted, aes(x = variable, y = value)) + geom_bar(stat = "identity",position = "dodge") +
  labs(x='YouTube-8M defined category name', y="Number of video ids", title='Number of Video IDs available per Category') +
  coord_flip() + geom_text(aes(label=value)) +
  theme(axis.text.x = element_blank(),axis.ticks = element_blank())

sum(nrow(beauty_fitness),nrow(books_literature),nrow(business_industrial),nrow(computers_electronics),nrow(food_drink))
```

```{r}
# DISCLAIMER: We've tried but it is not working to open direct from HDFS.
library("httr")
# Parameters
hdfsUri <- "https://10.0.2.15:50470"
fileUri <- "/home/data/video-likes.zip"
readParameter <- "?op=OPEN"
optionnalParameters <- ""
uri <- paste0(hdfsUri, fileUri, readParameter, optionnalParameters)
data <- read.delim(uri)
```

# Analysis on the Video Ids that we obtained Transcripts but we didn't obtain Metadata

```{r}
cl <- makeCluster(detectCores() - 1)
json_like_files<-list.files(path =paste0(root_path,"Files/video-likes"),pattern="*.json",full.names = TRUE)
json_like_list<-parLapply(cl,json_like_files,function(x) rjson::fromJSON(file=x,method = "R"))
stopCluster(cl)
#Code to add the id to the list
N <- length(json_like_files)
for (i in 1:N){
  print(i)
  filename <- basename(json_like_files[i]) %>% gsub(pattern = ".json",replacement = "",.)
  names(json_like_list[[i]])[length(json_like_list[[i]])] <- "video_id"
  json_like_list[[i]]$video_id <- filename
}

#Transform list into dataframe to be easier to plot
library(dplyr)

dataframeLikes <- bind_rows(lapply(json_like_list, as.data.frame.list))
dataframeLikes$viewCount <- as.numeric(dataframeLikes$viewCount)
dataframeLikes$likeCount <- as.numeric(dataframeLikes$likeCount)
dataframeLikes$dislikeCount <- as.numeric(dataframeLikes$dislikeCount)

#Summary
summary(dataframeLikes)

#Boxplot - it does not look good
op<-par(mfrow=c(1,2))

boxplot(dataframeLikes$viewCount, names = "View Count",las = 2, col = "royalblue")
mtext("Count", side = 2, line = 3)

boxplot(dataframeLikes$likeCount, names = "Like Count",las = 2, col = "royalblue")
mtext("Count", side = 2, line = 3)

boxplot(dataframeLikes$dislikeCount, names = "Dislike Count",las = 2, col = "royalblue")
mtext("Count", side = 2, line = 3)

par(op)

# Check the ids in the transcript files that are not present in the video metadata
transcript_files<-list.files(path =paste0(root_path,"Files/ALL_Transcripts"),pattern="*.txt",full.names = TRUE)
#Get the video ids that have transcript
for (i in 1:length((transcript_files))){
  filename <- basename(transcript_files[i]) %>% gsub(pattern = ".en.txt",replacement = "",.)
  transcript_video_ids <- rbind(transcript_video_ids,filename)
}

#Get video ids that have video metadata
for (i in 1:length((json_files))){
  filename <- basename(json_files[i]) %>% gsub(pattern = ".info.json",replacement = "",.)
  json_video_ids <- rbind(json_video_ids,filename)
}

#Get the valuesin transcript_video_ids but not in json_video_ids
diff_lists<-setdiff(transcript_video_ids,json_video_ids)
diff_lists <- as.data.frame(diff_lists)

diff_lists$inbeautyfitness<- ifelse(diff_lists$diff_lists %in% beauty_fitness$V1,"Yes", "No")
new_beauty_fitness <- diff_lists %>% filter(inbeautyfitness == "Yes")

diff_lists$inbooksliterature<- ifelse(diff_lists$diff_lists %in% books_literature$V1,"Yes", "No")
new_books_literature <- diff_lists %>% filter(inbooksliterature == "Yes")

diff_lists$inbusinessindustrial<- ifelse(diff_lists$diff_lists %in% business_industrial$V1,"Yes", "No")
new_business_industrial <- diff_lists %>% filter(inbusinessindustrial == "Yes")

diff_lists$incomputer<- ifelse(diff_lists$diff_lists %in% computers_electronics$V1,"Yes", "No")
new_computers_electronics <- diff_lists %>% filter(incomputer == "Yes")

diff_lists$infooddrink<- ifelse(diff_lists$diff_lists %in% food_drink$V1,"Yes", "No")
new_food_drink <- diff_lists %>% filter(infooddrink == "Yes")

diff_lists_balance<-data.frame(nrow(new_beauty_fitness),nrow(new_books_literature),nrow(new_business_industrial),nrow(new_computers_electronics),nrow(new_food_drink))
colnames(diff_lists_balance) <- c("Beauty & Fitness","Books & Literature", "Business & Industrial","Computers & Electronics","Food & Drink")

library('data.table')
dat.m <- melt(as.data.table(diff_lists_balance, keep.rownames = "Vars"), id.vars = "Vars")

ggplot(dat.m, aes(x = variable, y = value)) + geom_bar(stat = "identity",position = "dodge") +
  labs(x='YouTube-8M defined category name', y="number of video ids", title='Video IDs with Transcript but without Metadata (per Category)') +
  coord_flip()

#Code to check whether there were videos that we didn't manage to get the like and dislike counts
#video_likes <- read.delim(paste0(root_path,"Files/video-likes/video-likes.txt"),header=FALSE,sep=" ",dec=" ")
#definitive_ids$invideolikes <- ifelse(definitive_ids$V1 %in% video_likes$V1,"Yes", "No")
#new_video_likes <- definitive_ids %>% filter(invideolikes == "No")
#write.table(new_video_likes$V1,file = "video-likes-diff.txt", sep = " ", row.names = FALSE, col.names = FALSE)
```
```{r}
df_videos_without_metadata <- dataframeLikes[dataframeLikes$video_id %in% diff_lists$diff_lists,]
summary(df_videos_without_metadata)
```
# Analysis on the Video Ids that we obtained Transcripts but we didn't obtain Sentiment Analysis calculation
```{r}
sentAnalysis <- read.csv(paste0(root_path,"youtubeProjectTFM/src/sentiment_analysis.csv"),sep=",",header = T)
diff_lists_t_sa<-setdiff(transcript_video_ids,sentAnalysis$video_id)
diff_lists_t_sa <- as.data.frame(diff_lists_t_sa)

diff_lists_t_sa$inbeautyfitness<- ifelse(diff_lists_t_sa$diff_lists_t_sa %in% beauty_fitness$V1,"Yes", "No")
new_beauty_fitness <- diff_lists_t_sa %>% filter(inbeautyfitness == "Yes")

diff_lists_t_sa$inbooksliterature<- ifelse(diff_lists_t_sa$diff_lists_t_sa %in% books_literature$V1,"Yes", "No")
new_books_literature <- diff_lists_t_sa %>% filter(inbooksliterature == "Yes")

diff_lists_t_sa$inbusinessindustrial<- ifelse(diff_lists_t_sa$diff_lists_t_sa %in% business_industrial$V1,"Yes", "No")
new_business_industrial <- diff_lists_t_sa %>% filter(inbusinessindustrial == "Yes")

diff_lists_t_sa$incomputer<- ifelse(diff_lists_t_sa$diff_lists_t_sa %in% computers_electronics$V1,"Yes", "No")
new_computers_electronics <- diff_lists_t_sa %>% filter(incomputer == "Yes")

diff_lists_t_sa$infooddrink<- ifelse(diff_lists_t_sa$diff_lists_t_sa %in% food_drink$V1,"Yes", "No")
new_food_drink <- diff_lists_t_sa %>% filter(infooddrink == "Yes")

diff_lists_balance_t_sa<-data.frame(nrow(new_beauty_fitness),nrow(new_books_literature),nrow(new_business_industrial),nrow(new_computers_electronics),nrow(new_food_drink))
colnames(diff_lists_balance_t_sa) <- c("Beauty & Fitness","Books & Literature", "Business & Industrial","Computers & Electronics","Food & Drink")

library('data.table')
dat.m <- melt(as.data.table(diff_lists_balance_t_sa, keep.rownames = "Vars"), id.vars = "Vars")

ggplot(dat.m, aes(x = variable, y = value)) + geom_bar(stat = "identity",position = "dodge") +
  labs(x='YouTube-8M defined category name', y="number of video ids", title='Video IDs with Transcript but without Sentiment Analysis (per Category)') +
  coord_flip()
```

```{r}
df_videos_without_sentAnalysis <- dataframeLikes[dataframeLikes$video_id %in% diff_lists_t_sa$diff_lists_t_sa,]
summary(df_videos_without_sentAnalysis)
```

## Annex: Save the files in a permanent data in order to be easily accessible
```{r}
#To almost mimic the functionality of store or stores in R, you can do the following. Use save(x,y,z,file="Permdata") to save permanent objects in "permdata". When you exit R, do not save the workspace. Then all temporary objects will disappear. In your .Rprofile put the command load("Permdata") so that the next time you invoke R the permanent objects will be available.
save(json_list,df_language_fulltitle,df_language_fulltitle_textcat, file="Permanentdata")
# Load the saved data
load("Permanentdata")
```